{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13150468,"sourceType":"datasetVersion","datasetId":8331960}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\n# V√©rifier que les fichiers sont bien l√† (optionnel mais recommand√©)\ndataset_dir = Path('/kaggle/input/augmented-savi-640/Dataset_B_640x640')\nworking_dir = Path('/kaggle/working/')\nprint(\"Contenu du dossier :\")\n!ls {dataset_dir}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Cr√©ation du Fichier YAML ---\n\n# Contenu du fichier de configuration.\n# Le 'path' doit pointer vers le dossier racine du dataset.\n# Les chemins 'train', 'val', 'test' sont relatifs √† ce 'path'.\nyaml_content = f\"\"\"\npath: {dataset_dir.as_posix()}\ntrain: images/train\nval: images/val\ntest: images/test\n\nnames:\n  0: Person\n  1: Bicycle\n  2: Car\n  3: Cattle\n\"\"\"\n\n# √âcriture du contenu dans un fichier .yaml dans le r√©pertoire de travail\nyaml_file_path = working_dir / 'dataset.yaml'\nwith open(yaml_file_path, 'w') as f:\n    f.write(yaml_content)\n\nprint(f\"Fichier de configuration cr√©√© avec succ√®s √† l'emplacement : {yaml_file_path}\")\nprint(\"\\n--- Contenu du YAML ---\")\n!cat {yaml_file_path}","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Chargez votre fichier CSV.\nmetadata_path = dataset_dir / 'metadata_640x640.csv'\ndf = pd.read_csv(metadata_path)\n\nprint(\"--- 5 premi√®res lignes du DataFrame ---\")\ndisplay(df.head())\n\nprint(\"\\n--- Informations g√©n√©rales sur le DataFrame ---\")\ndf.info()\n\nprint(\"\\n--- Statistiques descriptives des colonnes num√©riques ---\")\ndisplay(df.describe())\n\nprint(\"\\n--- Valeurs uniques dans les colonnes cat√©gorielles ---\")\nprint(f\"Meteo: {df['meteo'].unique()}\")\nprint(f\"Region: {df['region'].unique()}\")\nprint(f\"Mode: {df['mode'].unique()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1) si l'id contient \"Tankpe\" -> region = \"urban periphery\"\ndf.loc[df['id'].str.contains('Tankpe', case=False, na=False), 'region'] = 'urban periphery'\n\n# 2) si l'id contient \"Godomey\" -> region = \"urban\"\ndf.loc[df['id'].str.contains('Godomey', case=False, na=False), 'region'] = 'urban'\n\n# 3) normaliser la colonne meteo : \"Sunny\" -> \"sunny\" et \"Night\" -> \"night\"\n# m√©thode robuste : enlever espaces puis tout mettre en minuscules\ndf['meteo'] = df['meteo'].astype(str).str.strip().str.lower()\n\n# v√©rifications rapides\nprint(\"Valeurs uniques dans 'region' apr√®s modifs :\", df['region'].unique())\nprint(\"Valeurs uniques dans 'meteo' apr√®s modifs  :\", df['meteo'].unique())\n\n# (optionnel) afficher quelques lignes concern√©es pour contr√¥le\nprint(\"\\nExemples d'entr√©es contenant 'Tankpe' :\")\nprint(df[df['id'].str.contains('Tankpe', case=False, na=False)].head())\n\nprint(\"\\nExemples d'entr√©es contenant 'Godomey' :\")\nprint(df[df['id'].str.contains('Godomey', case=False, na=False)].head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# S√©lection des colonnes cat√©gorielles √† encoder\ncategorical_cols = ['meteo', 'region', 'mode']\n\n# Application de l'encodage one-hot\ndf_encoded = pd.get_dummies(df, columns=categorical_cols, prefix=categorical_cols)\n\nprint(\"--- DataFrame apr√®s encodage one-hot ---\")\ndisplay(df_encoded.head())\n\n# Garder en m√©moire les colonnes cr√©√©es pour pouvoir les r√©utiliser √† l'inf√©rence\nencoded_cols = [col for col in df_encoded.columns if any(cat_col in col for cat_col in categorical_cols)]\nprint(f\"\\nColonnes cr√©√©es par l'encodage : {encoded_cols}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nimport pickle\n\n# S√©lection des colonnes num√©riques √† normaliser\nnumerical_cols = ['angle', 'altitude', 'y_start', 'y_end']\n\n# Initialisation du scaler Min-Max\nscaler = MinMaxScaler()\n\n# Application du scaler sur nos donn√©es\ndf_encoded[numerical_cols] = scaler.fit_transform(df_encoded[numerical_cols])\n\nprint(\"--- DataFrame apr√®s normalisation des donn√©es num√©riques ---\")\ndisplay(df_encoded.head())\n\n# --- CRUCIAL : Sauvegarde du scaler ---\n# Nous en aurons besoin plus tard pour transformer les donn√©es de validation/test\n# avec EXACTEMENT la m√™me √©chelle apprise sur les donn√©es d'entra√Ænement.\nscaler_path = '/kaggle/working/min_max_scaler.pkl'\nwith open(scaler_path, 'wb') as f:\n    pickle.dump(scaler, f)\n\nprint(f\"\\nScaler sauvegard√© √† l'emplacement : {scaler_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Mettre la colonne 'id' comme index pour une recherche facile plus tard\ndf_processed = df_encoded.set_index('id')\n\nprint(\"--- DataFrame final pr√™t pour l'entra√Ænement ---\")\ndisplay(df_processed.head())\n\nprint(\"\\n--- Dimensions du vecteur de caract√©ristiques pour le MLP ---\")\nprint(f\"Chaque image sera repr√©sent√©e par un vecteur de {df_processed.shape[1]} features.\")\n\n# Sauvegarder le DataFrame trait√© pour une utilisation future\nprocessed_data_path = '/kaggle/working/processed_metadata.csv'\ndf_processed.to_csv(processed_data_path)\n\nprint(f\"\\nDonn√©es trait√©es sauvegard√©es √† l'emplacement : {processed_data_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Installation ---\n# On installe la biblioth√®que ultralytics qui contient l'impl√©mentation de YOLOv8.\n# Le flag '-q' (quiet) permet de r√©duire la quantit√© de logs durant l'installation.\n!pip install ultralytics -q\n\nprint(\"Installation termin√©e.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport cv2\nimport numpy as np\n\nclass MultimodalDataset(Dataset):\n    \"\"\"\n    Dataset PyTorch personnalis√© pour charger des images, leurs labels YOLO,\n    et des m√©tadonn√©es tabulaires associ√©es.\n    \"\"\"\n    def __init__(self, images_dir, labels_dir, metadata_df):\n        \"\"\"\n        Args:\n            images_dir (str): Chemin vers le dossier contenant les images.\n            labels_dir (str): Chemin vers le dossier contenant les fichiers de labels (.txt).\n            metadata_df (pd.DataFrame): DataFrame contenant les m√©tadonn√©es pr√©trait√©es.\n                                        L'index du DataFrame doit √™tre l'ID de l'image.\n        \"\"\"\n        self.images_dir = Path(images_dir)\n        self.labels_dir = Path(labels_dir)\n        self.metadata_df = metadata_df\n        \n        # Obtenir tous les noms de fichiers image (sans extension)\n        all_image_stems = {p.stem for p in self.images_dir.glob('*.jpg')}\n        \n        # Filtrer pour ne garder que les IDs qui ont une entr√©e dans le metadata_df\n        self.image_ids = sorted([\n            stem for stem in all_image_stems\n            if stem in self.metadata_df.index\n        ])\n        \n        # Avertissement si des images n'ont pas de m√©tadonn√©es\n        if len(all_image_stems) != len(self.image_ids):\n            missing_count = len(all_image_stems) - len(self.image_ids)\n            print(f\"Attention : {missing_count} images dans {images_dir} n'ont pas de m√©tadonn√©es correspondantes et seront ignor√©es.\")\n\n\n    def __len__(self):\n        \"\"\"Retourne le nombre total d'√©chantillons dans le dataset.\"\"\"\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        R√©cup√®re un √©chantillon (image, labels, m√©tadonn√©es) √† l'index donn√©.\n        \"\"\"\n        # 1. Obtenir l'ID de l'image\n        image_id = self.image_ids[idx]\n        \n        # 2. Charger l'image\n        image_path = self.images_dir / f\"{image_id}.jpg\"\n        image = cv2.imread(str(image_path))\n        target_size = (640, 640)\n        if image.shape[:2] != (target_size[1], target_size[0]):\n             image = cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image_tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n        \n        # 3. Charger les labels\n        labels = []\n        label_path = self.labels_dir / f\"{image_id}.txt\"\n        if label_path.exists():\n            with open(label_path, 'r') as f:\n                for line in f.readlines():\n                    parts = line.strip().split()\n                    labels.append([float(p) for p in parts])\n        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n        \n        # 4. R√©cup√©rer les m√©tadonn√©es\n        metadata_vector = self.metadata_df.loc[image_id].values.astype(np.float32)\n        metadata_tensor = torch.from_numpy(metadata_vector)\n        \n        # 5. Retourner un dictionnaire\n        return {\n            'image': image_tensor,\n            'labels': labels_tensor,\n            'metadata': metadata_tensor,\n            'id': image_id\n        }\n\nprint(\"Classe MultimodalDataset d√©finie avec succ√®s.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Configuration des Chemins ---\nBASE_DATA_DIR = Path('/kaggle/input/augmented-savi-640/Dataset_B_640x640') # D'apr√®s votre notebook\nMETADATA_PATH = '/kaggle/working/processed_metadata.csv' # Le fichier que nous avons cr√©√© √† l'√©tape 1\n\n# D√©finir les chemins sp√©cifiques pour chaque sous-ensemble\nimages_train_dir = BASE_DATA_DIR / 'images' / 'train'\nlabels_train_dir = BASE_DATA_DIR / 'labels' / 'train'\n\nimages_val_dir = BASE_DATA_DIR / 'images' / 'val'\nlabels_val_dir = BASE_DATA_DIR / 'labels' / 'val'\n\nimages_test_dir = BASE_DATA_DIR / 'images' / 'test'\nlabels_test_dir = BASE_DATA_DIR / 'labels' / 'test'\n\n# --- Chargement des M√©tadonn√©es ---\ndf_processed = pd.read_csv(METADATA_PATH, index_col='id')\nprint(f\"M√©tadonn√©es charg√©es avec {len(df_processed)} entr√©es.\")\n\n# --- Instanciation des Datasets ---\nprint(\"\\nInstanciation des datasets...\")\n\ntrain_dataset = MultimodalDataset(\n    images_dir=images_train_dir,\n    labels_dir=labels_train_dir,\n    metadata_df=df_processed\n)\n\nval_dataset = MultimodalDataset(\n    images_dir=images_val_dir,\n    labels_dir=labels_val_dir,\n    metadata_df=df_processed\n)\n\ntest_dataset = MultimodalDataset(\n    images_dir=images_test_dir,\n    labels_dir=labels_test_dir,\n    metadata_df=df_processed\n)\n\n# --- V√©rification ---\nprint(\"\\n--- V√©rification des tailles des datasets ---\")\nprint(f\"Nombre d'√©chantillons dans le set d'entra√Ænement : {len(train_dataset)}\")\nprint(f\"Nombre d'√©chantillons dans le set de validation   : {len(val_dataset)}\")\nprint(f\"Nombre d'√©chantillons dans le set de test         : {len(test_dataset)}\")\n\n# --- Test sur un √©chantillon du set de validation ---\nif len(val_dataset) > 0:\n    print(\"\\n--- Test sur le premier √©chantillon du set de validation ---\")\n    \n    sample = val_dataset[0]\n    \n    print(f\"ID de l'image : {sample['id']}\")\n    print(f\"Cl√©s retourn√©es : {list(sample.keys())}\")\n    \n    img_tensor = sample['image']\n    lbl_tensor = sample['labels']\n    meta_tensor = sample['metadata']\n    \n    print(f\"Image - Shape: {img_tensor.shape}, Type: {img_tensor.dtype}\")\n    print(f\"Labels - Shape: {lbl_tensor.shape}, Type: {lbl_tensor.dtype}\")\n    print(f\"Metadata - Shape: {meta_tensor.shape}, Type: {meta_tensor.dtype}\")\n    \n    # V√©rifiez que le nombre de features des m√©tadonn√©es correspond bien\n    expected_features = df_processed.shape[1]\n    print(f\"Le vecteur de m√©tadonn√©es a {meta_tensor.shape[0]} features (attendu: {expected_features}).\")\n\nelse:\n    print(\"\\nAttention : Le dataset de validation est vide. Veuillez v√©rifier les chemins d'acc√®s.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass MLP(nn.Module):\n    \"\"\"\n    Un Multi-Layer Perceptron simple pour traiter les m√©tadonn√©es tabulaires.\n    \"\"\"\n    def __init__(self, input_size, output_size=512):\n        \"\"\"\n        Args:\n            input_size (int): La taille du vecteur de m√©tadonn√©es d'entr√©e.\n            output_size (int): La taille du vecteur de caract√©ristiques en sortie (embedding).\n        \"\"\"\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(input_size, 128),\n            nn.ReLU(),\n            nn.Dropout(0.1), # Ajout de dropout pour la r√©gularisation\n            nn.Linear(128, 256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, output_size)\n        )\n\n    def forward(self, x):\n        \"\"\"Passe avant du MLP.\"\"\"\n        return self.layers(x)\n\n# --- Test rapide du MLP ---\n# R√©cup√©rer la taille d'entr√©e depuis nos donn√©es pr√©trait√©es\nmetadata_df = pd.read_csv('/kaggle/working/processed_metadata.csv')\ninput_features = metadata_df.shape[1] - 1 # -1 car la colonne 'id' est l'index\n\n# Instancier le MLP\nmlp_model = MLP(input_size=input_features)\n\n# Cr√©er un faux tenseur de m√©tadonn√©es (batch de 4)\ndummy_metadata = torch.randn(4, input_features)\n\n# Faire une passe avant\noutput_embedding = mlp_model(dummy_metadata)\n\nprint(f\"--- Test du MLP ---\")\nprint(f\"Taille du vecteur d'entr√©e : {input_features}\")\nprint(f\"Shape de l'entr√©e du MLP : {dummy_metadata.shape}\")\nprint(f\"Shape de la sortie (embedding) du MLP : {output_embedding.shape}\") # Devrait √™tre [4, 512]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass ChannelAttention(nn.Module):\n    \"\"\"Channel-attention module https://github.com/open-mmlab/mmdetection/tree/v3.0.0rc1/configs/rtmdet.\"\"\"\n\n    def __init__(self, channels: int) -> None:\n        \"\"\"Initializes the class and sets the basic configurations and instance variables required.\"\"\"\n        super().__init__()\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Conv2d(channels, channels, 1, 1, 0, bias=True)\n        self.act = nn.Sigmoid()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Applies forward pass using activation on convolutions of the input, optionally using batch normalization.\"\"\"\n        return x * self.act(self.fc(self.pool(x)))\n\n\nclass SpatialAttention(nn.Module):\n    \"\"\"Spatial-attention module.\"\"\"\n\n    def __init__(self, kernel_size=7):\n        \"\"\"Initialize Spatial-attention module with kernel size argument.\"\"\"\n        super().__init__()\n        assert kernel_size in {3, 7}, \"kernel size must be 3 or 7\"\n        padding = 3 if kernel_size == 7 else 1\n        self.cv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n        self.act = nn.Sigmoid()\n\n    def forward(self, x):\n        \"\"\"Apply channel and spatial attention on input for feature recalibration.\"\"\"\n        return x * self.act(self.cv1(torch.cat([torch.mean(x, 1, keepdim=True), torch.max(x, 1, keepdim=True)[0]], 1)))\n\n\nclass CBAM(nn.Module):\n    \"\"\"Convolutional Block Attention Module.\"\"\"\n\n    def __init__(self, c1, kernel_size=7):\n        \"\"\"Initialize CBAM with given input channel (c1) and kernel size.\"\"\"\n        super().__init__()\n        self.channel_attention = ChannelAttention(c1)\n        self.spatial_attention = SpatialAttention(kernel_size)\n\n    def forward(self, x):\n        \"\"\"Applies the forward pass through C1 module.\"\"\"\n        return self.spatial_attention(self.channel_attention(x))\n\nprint(\"Module CBAM d√©finis avec succ√®s.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- √âtape 1 : Importer le parseur de mod√®les ---\nfrom ultralytics.nn import tasks\n\n# --- Enregistrer notre module personnalis√© ---\ntasks.CBAM = CBAM\nprint(\"Module CBAM enregistr√© avec succ√®s.\")\n\n# --- Cr√©ation du Fichier de Configuration YAML Final ---\n\nyaml_config_content = \"\"\"\n# Ultralytics YOLO üöÄ, AGPL-3.0 license\n# Fichier de configuration pour YOLOv8s avec des blocs C2f_CBAM\n\n# Param√®tres\nnc: 4 \nscales:\n  # [depth, width, max_channels]\n  s: [0.33, 0.50, 1024]  #\n\nbackbone:\n  # [from, repeats, module, args]\n  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2\n  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4\n  - [-1, 3, C2f, [128, True]]\n  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8\n  - [-1, 6, C2f, [256, True]]\n  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16\n  - [-1, 6, C2f, [512, True]]\n  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32\n  - [-1, 3, C2f, [1024, True]]\n  - [-1, 1, SPPF, [1024, 5]]  # 9\n\nhead:\n  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]  # 10\n  - [-1, 1, CBAM, [512]]  # Add CBAM after Upsample\n  - [[-1, 6], 1, Concat, [1]]  # 12 cat backbone P4\n  - [-1, 3, C2f, [512, False]]  # 13\n\n  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]  # 14\n  - [-1, 1, CBAM, [256]]  # Add CBAM after Upsample\n  - [[-1, 4], 1, Concat, [1]]  # 16 cat backbone P3\n  - [-1, 3, C2f, [256, False]]  # 17\n\n  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]  # 18\n  - [-1, 1, CBAM, [128]]  # Add CBAM after Upsample\n  - [[-1, 2], 1, Concat, [1]]  # 20 cat backbone P2\n  - [-1, 1, C2f, [128, False]]  # 21\n\n  - [-1, 1, Conv, [128, 3, 2]]  # 22\n  - [[-1, 17], 1, Concat, [1]]  # 23 cat head P3\n  - [-1, 3, C2f, [256, False]]  # 24\n\n  - [-1, 1, Conv, [256, 3, 2]]  # 25\n  - [[-1, 13], 1, Concat, [1]]  # 26 cat head P4\n  - [-1, 3, C2f, [512, False]]  # 27\n\n  - [-1, 1, Conv, [512, 3, 2]]  # 28\n  - [[-1, 9], 1, Concat, [1]]  # 29 cat head P5\n  - [-1, 3, C2f, [1024, False]]  # 30\n\n  - [[21, 24, 27, 30], 1, Detect, [nc]]  # 31 Detect(P2, P3, P4, P5)\n\"\"\"\n\n# √âcrire ce contenu dans un fichier .yaml dans le r√©pertoire de travail\ncustom_yaml_path = working_dir / 'yolov8s-cbam.yaml'\nwith open(custom_yaml_path, 'w') as f:\n    f.write(yaml_config_content)\n\nprint(f\"Fichier de configuration YAML personnalis√© cr√©√© : {custom_yaml_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics.nn.tasks import DetectionModel\nfrom ultralytics.nn.modules import Concat, C2f, Conv\n\nclass YOLOv8Multimodal(nn.Module):\n    \"\"\"\n    Mod√®le multimodal qui fusionne les caract√©ristiques d'un backbone YOLOv8\n    avec des m√©tadonn√©es via un MLP. (Version corrig√©e)\n    \"\"\"\n    def __init__(self, yolo_cfg_path, metadata_input_size, num_classes):\n        super().__init__()\n        \n        # 1. Charger le mod√®le YOLO de base.\n        self.yolo_model = DetectionModel(cfg=yolo_cfg_path, nc=num_classes)\n\n        self.model = self.yolo_model.model\n        \n        # 2. Isoler la t√™te de d√©tection. C'est notre source de v√©rit√©.\n        self.detect_head = self.model[-1]\n        \n        # 3. Instancier notre MLP\n        metadata_embedding_size = 512\n        self.metadata_mlp = MLP(input_size=metadata_input_size, output_size=metadata_embedding_size)\n        \n        # 4. --- SOLUTION CORRIG√âE : Acc√®s correct aux propri√©t√©s des couches ---\n        self.fusion_indices = [21, 24, 27, 30]\n        self.fusion_convs = nn.ModuleList()\n        \n        print(\"D√©termination dynamique des canaux en inspectant la t√™te 'Detect'...\")\n        \n        # self.detect_head.nl est le nombre de couches de d√©tection (4 dans notre cas)\n        for i in range(self.detect_head.nl):\n            # CORRECTION : Acc√©der correctement aux propri√©t√©s de la convolution\n            # La classe Conv d'Ultralytics a un attribut 'conv' qui contient la vraie Conv2d de PyTorch\n            try:\n                # M√©thode 1 : Essayer d'acc√©der via l'attribut conv\n                if hasattr(self.detect_head.cv2[i][0], 'conv'):\n                    image_channels = self.detect_head.cv2[i][0].conv.in_channels\n                # M√©thode 2 : Essayer d'acc√©der directement si c'est d√©j√† une Conv2d\n                elif hasattr(self.detect_head.cv2[i][0], 'in_channels'):\n                    image_channels = self.detect_head.cv2[i][0].in_channels\n                # M√©thode 3 : Inspection des param√®tres du module\n                else:\n                    # R√©cup√©rer les param√®tres du premier module Conv\n                    conv_module = self.detect_head.cv2[i][0]\n                    # Les modules Conv d'Ultralytics stockent leurs param√®tres diff√©remment\n                    for name, param in conv_module.named_parameters():\n                        if 'weight' in name:\n                            image_channels = param.shape[1]  # in_channels est la 2√®me dimension\n                            break\n                    else:\n                        # Fallback : utiliser une valeur par d√©faut bas√©e sur l'index\n                        default_channels = [64, 128, 256, 512]\n                        image_channels = default_channels[i] if i < len(default_channels) else 512\n                        print(f\"  - Attention: Utilisation de la valeur par d√©faut pour la branche {i}: {image_channels} canaux\")\n                        \n            except Exception as e:\n                # En cas d'erreur, utiliser des valeurs par d√©faut raisonnables\n                default_channels = [64, 128, 256, 512]\n                image_channels = default_channels[i] if i < len(default_channels) else 512\n                print(f\"  - Erreur lors de l'inspection de la branche {i}: {e}\")\n                print(f\"  - Utilisation de la valeur par d√©faut: {image_channels} canaux\")\n            \n            print(f\"  - Branche {i} (entr√©e de la couche {self.fusion_indices[i]}): {image_channels} canaux d'image requis.\")\n            \n            # Cr√©er la couche de fusion correspondante avec les bonnes dimensions\n            fusion_layer = self._create_fusion_layer(image_channels, metadata_embedding_size)\n            self.fusion_convs.append(fusion_layer)\n\n    def _create_fusion_layer(self, image_channels, metadata_channels):\n        \"\"\"Cr√©e une petite couche de convolution pour r√©duire la dimension apr√®s la fusion.\"\"\"\n        return nn.Sequential(\n            nn.Conv2d(image_channels + metadata_channels, image_channels, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(image_channels),\n            nn.SiLU()\n        )\n\n    def forward(self, image, metadata):\n        \"\"\"\n        La passe avant du mod√®le multimodal.\n        \"\"\"\n        metadata_embedding = self.metadata_mlp(metadata)\n\n        y = []\n        fusion_sources = {}\n        for i, module in enumerate(self.model[:-1]):\n            if module.f == -1:\n                x = y[-1] if y else image\n            else:\n                x = [y[j] for j in module.f]\n            \n            x = module(x)\n            y.append(x)\n            \n            if i in self.fusion_indices:\n                fusion_sources[i] = x\n        \n        yolo_outputs = [fusion_sources[i] for i in self.fusion_indices]\n        fused_features = []\n\n        for yolo_out, fusion_conv in zip(yolo_outputs, self.fusion_convs):\n            b, c, h, w = yolo_out.shape\n            meta_emb = metadata_embedding.unsqueeze(-1).unsqueeze(-1).expand(b, -1, h, w)\n            fused_out = torch.cat([yolo_out, meta_emb], dim=1)\n            fused_features.append(fusion_conv(fused_out))\n        \n        return self.detect_head(fused_features)\n\nprint(\"Classe YOLOv8Multimodal (version corrig√©e) d√©finie avec succ√®s.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Param√®tres de configuration ---\nYOLO_CFG_PATH = '/kaggle/working/yolov8s-cbam.yaml' # Le YAML que vous avez cr√©√©\nMETADATA_INPUT_SIZE = input_features # Calcul√© dans la cellule 3.1\nNUM_CLASSES = 4 # Person, Bicycle, Car, Cattle\n\n# --- Instanciation du mod√®le complet ---\ntry:\n    multimodal_model = YOLOv8Multimodal(\n        yolo_cfg_path=YOLO_CFG_PATH,\n        metadata_input_size=METADATA_INPUT_SIZE,\n        num_classes=NUM_CLASSES\n    )\n    print(\"Mod√®le multimodal instanci√© avec succ√®s.\")\n    \n    # --- Cr√©ation de donn√©es d'entr√©e factices ---\n    BATCH_SIZE = 2\n    IMG_SIZE = 640\n    dummy_images = torch.randn(BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE)\n    dummy_metadata = torch.randn(BATCH_SIZE, METADATA_INPUT_SIZE)\n    \n    # Mettre le mod√®le en mode √©valuation pour le test\n    multimodal_model.eval()\n    \n    # --- Passe avant ---\n    with torch.no_grad():\n        print(\"\\nEx√©cution d'une passe avant (dry run)...\")\n        predictions = multimodal_model(dummy_images, dummy_metadata)\n    \n    print(\"Passe avant r√©ussie !\")\n    \n    # --- Analyse de la sortie ---\n    # La sortie de la t√™te de d√©tection de YOLOv8 est une liste de tenseurs\n    # (un pour chaque √©chelle de pr√©diction).\n    print(f\"\\nType de la sortie : {type(predictions)}\")\n    print(f\"Nombre de tenseurs en sortie : {len(predictions)}\")\n    \n    # Le premier tenseur contient les pr√©dictions (bo√Ætes, scores de classe, score de confiance)\n    # Sa shape est [batch_size, num_classes + 4 (pour la bo√Æte), num_predictions]\n    print(f\"Shape du premier tenseur de pr√©diction : {predictions[0].shape}\")\n    \nexcept Exception as e:\n    print(f\"\\nUne erreur est survenue lors de l'instanciation ou du test du mod√®le : {e}\")\n    import traceback\n    traceback.print_exc()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def freeze_yolo_backbone(model):\n    \"\"\"G√®le tous les poids du backbone yolo_model.\"\"\"\n    print(\"Gel des poids du backbone YOLO...\")\n    for name, param in model.named_parameters():\n        if 'yolo_model' in name:\n            param.requires_grad = False\n\ndef unfreeze_yolo_backbone(model):\n    \"\"\"D√©g√®le tous les poids du backbone yolo_model.\"\"\"\n    print(\"D√©gel des poids du backbone YOLO...\")\n    for name, param in model.named_parameters():\n        if 'yolo_model' in name:\n            param.requires_grad = True\n\ndef check_frozen_status(model):\n    \"\"\"V√©rifie et affiche le statut (gel√©/d√©gel√©) des diff√©rents groupes de param√®tres.\"\"\"\n    print(\"\\n--- Statut des Param√®tres ---\")\n    status = {\"yolo_model\": True, \"metadata_mlp\": False, \"fusion_convs\": False}\n    for name, param in model.named_parameters():\n        group = name.split('.')[0]\n        if group not in status:\n            status[group] = param.requires_grad\n        else:\n            status[group] = status[group] and param.requires_grad\n    \n    for group, is_trainable in status.items():\n        print(f\"  - Groupe '{group}': {'Entra√Ænable' if is_trainable else 'Gel√©'}\")\n    print(\"----------------------------\\n\")\n\nprint(\"Fonctions de gel/d√©gel d√©finies.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport os\nfrom pathlib import Path\nimport yaml\nimport copy\n\n# Importer directement la classe de la fonction de perte\nfrom ultralytics.utils.loss import v8DetectionLoss\n\n# --- 1. Hyperparam√®tres et Configuration ---\nEPOCHS = 300\nBATCH_SIZE = 8\nLEARNING_RATE = 1e-3\nPROJECT_NAME = 'multimodal_runs_pure' # Nouveau nom pour ne pas tout m√©langer\nEXPERIMENT_NAME = 'exp_final'\n\n# Cr√©er le r√©pertoire de sauvegarde\nsave_dir = Path(f'/kaggle/working/{PROJECT_NAME}/{EXPERIMENT_NAME}')\nsave_dir.mkdir(parents=True, exist_ok=True)\nweights_dir = save_dir / 'weights'\nweights_dir.mkdir(exist_ok=True)\n\n# --- 2. Mod√®le, Optimiseur, Scheduler ---\n# (On suppose que les DataLoaders train_loader et val_loader existent d√©j√†)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Utilisation du device : {device}\")\n\nmultimodal_model.to(device)\n\n# Geler le backbone pour la Phase 1\nfreeze_yolo_backbone(multimodal_model)\ncheck_frozen_status(multimodal_model)\n\n# L'optimiseur ne voit que les param√®tres entra√Ænables ---\n# C'est la m√©thode standard pour un entra√Ænement avec des couches gel√©es.\ntrainable_params = filter(lambda p: p.requires_grad, multimodal_model.parameters())\noptimizer = torch.optim.AdamW(trainable_params, lr=LEARNING_RATE)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n\n# --- 3. Instanciation de la Fonction de Perte ---\n# On a besoin d'un objet 'args' factice pour la fonction de perte\nfrom types import SimpleNamespace\n# Ces valeurs sont les poids par d√©faut de la perte dans ultralytics\nargs = SimpleNamespace(box=7.5, cls=0.5, dfl=1.5) \nmultimodal_model.args = args\n\n# La perte a aussi besoin de conna√Ætre la t√™te de d√©tection\nmultimodal_model.model = multimodal_model.yolo_model.model\n\nloss_fn = v8DetectionLoss(multimodal_model)\n\nprint(\"Configuration pure termin√©e. Pr√™t pour l'entra√Ænement.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef validate_model(model, loader, loss_function, device):\n    \"\"\"\n    Fonction de validation simple qui calcule la perte moyenne sur l'ensemble de validation.\n    \"\"\"\n    model.eval()  # Passer le mod√®le en mode √©valuation\n    total_val_loss = 0.0\n    pbar_val = tqdm(loader, desc=\"[Validation]\")\n\n    with torch.no_grad():  # Pas de calcul de gradient pendant la validation\n        for batch in pbar_val:\n            images = batch['image'].to(device)\n            metadata = batch['metadata'].to(device)\n            targets = batch['labels'].to(device)\n            \n            # G√©rer le cas o√π un batch de validation n'a aucune cible\n            if targets.numel() == 0:\n                continue\n\n            preds = model(images, metadata)\n            \n            batch_for_loss = {\n                'imgs': images,\n                'batch_idx': targets[:, 0],\n                'cls': targets[:, 1],\n                'bboxes': targets[:, 2:]\n            }\n\n            loss, loss_items = loss_function(preds, batch_for_loss)\n            total_val_loss += loss.sum().item()\n            \n            pbar_val.set_postfix(val_loss=f'{total_val_loss / (pbar_val.n + 1):.4f}')\n            \n    return total_val_loss / len(loader)\n\nprint(\"Fonction de validation d√©finie.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_val_loss = float('inf')\nNUM_EPOCHS_FREEZE = 100 # Le nombre d'√©poques pour la Phase 1\n\n# Boucle principale sur les √©poques\nfor epoch in range(EPOCHS):\n    if epoch == NUM_EPOCHS_FREEZE:\n        unfreeze_yolo_backbone(multimodal_model)\n        check_frozen_status(multimodal_model)\n        \n        print(\"Phase 2 : D√©gel et cr√©ation d'un nouvel optimiseur avec un learning rate plus faible.\")\n        # On entra√Æne maintenant TOUS les param√®tres avec un LR plus faible\n        optimizer = torch.optim.AdamW(multimodal_model.parameters(), lr=LEARNING_RATE / 10)\n        # On peut optionnellement r√©initialiser le scheduler\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS - NUM_EPOCHS_FREEZE)\n    multimodal_model.train()\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Training]\")\n    total_train_loss = 0.0\n    \n    for i, batch in enumerate(pbar):\n        images = batch['image'].to(device)\n        metadata = batch['metadata'].to(device)\n        targets = batch['labels'].to(device)\n        \n        # Sauter les batchs sans aucune annotation\n        if targets.numel() == 0:\n            continue\n            \n        optimizer.zero_grad()\n        \n        preds = multimodal_model(images, metadata)\n        \n        batch_for_loss = {\n            'imgs': images,\n            'batch_idx': targets[:, 0],\n            'cls': targets[:, 1],\n            'bboxes': targets[:, 2:]\n        }\n\n        loss, loss_items = loss_fn(preds, batch_for_loss)\n        \n        # --- DEBUG : Afficher les composantes de la perte pour le premier batch ---\n        if i == 0:\n            print(f\"\\nComposantes de la perte (1er batch): {loss_items}\")\n            \n        loss_scalar = loss.sum()\n        loss_scalar.backward()\n        optimizer.step()\n        \n        total_train_loss += loss_scalar.item()\n        pbar.set_postfix(train_loss=f'{total_train_loss / (i + 1):.4f}')\n        \n    scheduler.step()\n\n    # --- Validation √† la fin de chaque √©poque ---\n    avg_val_loss = validate_model(multimodal_model, val_loader, loss_fn, device)\n    print(f\"\\nEpoch {epoch+1} - Perte d'entra√Ænement moyenne: {total_train_loss / len(train_loader):.4f} - Perte de validation moyenne: {avg_val_loss:.4f}\")\n\n    # --- Sauvegarde des mod√®les ---\n    model_to_save = multimodal_model.module if hasattr(multimodal_model, 'module') else multimodal_model\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model_to_save.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'val_loss': avg_val_loss\n    }\n\n    # Sauvegarder le dernier mod√®le\n    torch.save(checkpoint, weights_dir / 'last.pt')\n\n    # Sauvegarder le meilleur mod√®le (bas√© sur la perte de validation)\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        torch.save(checkpoint, weights_dir / 'best.pt')\n        print(f\"  -> Nouveau meilleur mod√®le sauvegard√© avec une perte de validation de : {avg_val_loss:.4f}\")\n        \nprint(\"\\n--- Entra√Ænement termin√© ! ---\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}